---
title: "Regularization Examples: Ridge Regression, LASSO, and Elastic Net"
author: "Andrew Tredennick"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    fig_caption: yes
vignette: >
  %\VignetteIndexEntry{Regularization Examples: Ridge Regression, LASSO, and Elastic Net}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This vignette walks through the code used to produce the examples of statistical regularization shown in the paper. We use the `butterfly` data from Roland and Matter 2016 (*Ecological Monographs*, data [here](https://datadryad.org/resource/doi:10.5061/dryad.tp324)). As mentioned in the paper, we focus on **ridge regression**, the **least absolute shrinkage and selector operator** (LASSO), and the **elastic net**, but many other regulators exist, and interested readers should peruse the machine learning literature. Outside of R, there is a comphrensive python library called [scikit-learn](http://scikit-learn.org/stable/index.html) with many model selection routines.
In what follows we use the `glmnet` package ([webpage](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html), [CRAN]https://cran.r-project.org/web/packages/glmnet/index.html) and show one example of the LASSO for mixed effects models using the `glmmLass` package ([CRAN]https://cran.r-project.org/web/packages/glmmLasso/index.html). 

##  Ridge regression
```{r, fig.width=6}
# Set seed and load packages ---------------------------------------------------
set.seed(12300)
library(modselr)
library(tidyverse)
library(glmnet) # package for statistical regularization


# Subset out fitting data ------------------------------------------------------
# Get one subpopulation
sub_pop <- butterfly %>%
  filter(meada == 'L', year < 2014)

# Subset out just the important variables from Roland & Matter (2016) Figure 3
imp_vars <- c("novextmax",
              "julraintmn1",
              "novextmin",
              "novmean",
              "decextmax",
              "novmeanmax",
              "logNt",
              "novmeanmin")


# Prepare data for glmnet ------------------------------------------------------
y <- sub_pop$Rt # population growth rate
X <- as.matrix(select(sub_pop, imp_vars)) # covariate matrix

# Make sure there are no NAs in the covariate matrix
test_vec <- as.numeric(apply(X, MARGIN = 2, FUN = "mean"))
nacols <- which(is.na(test_vec))
if(length(nacols) > 0) X <- X[,-nacols]

# Standaradize the covariate values
X <- scale(X, center = TRUE, scale = TRUE)


# Run ridge regression ---------------------------------------------------------
pen_facts <- rep(1, ncol(X)) # penalize all covariates
lambdas <- 10^seq(2, -2, by = -.005) # sequence of penalties to test

ridge_out <- cv.glmnet(x = X, 
                       y = y, 
                       lambda = lambdas,
                       penalty.factor = pen_facts,
                       family = "gaussian", 
                       alpha = 0, # 0 for ridge, 1 for lasso, 0.5 for elastic net 
                       standardize = FALSE, 
                       type.measure = "mse", # mean square error
                       nfolds = 6) # for cross-validation


# Collect results into data frames ---------------------------------------------
lambdas <- ridge_out$lambda
cv_scores <- ridge_out$cvm
all_coefs <- as.data.frame(as.matrix(t(ridge_out$glmnet.fit$beta)[,1:ncol(X)]))
colnames(all_coefs) <- colnames(X)
all_coefs <- all_coefs %>%
  mutate(lambda = log(lambdas)) %>%
  gather(covariate, value, -lambda)

mse_df <- data.frame(lambda = log(lambdas),
                     score = cv_scores)

best_lambda <- min(mse_df$lambda[which(mse_df$score == min(mse_df$score))])


# Plot the regularization paths ------------------------------------------------
# These are 'modselr' functions for plotting
make_coef_plot(coef_df = all_coefs, 
               best_lambda = best_lambda, 
               style = "base") + ggtitle("Ridge regression: coefficient paths")

make_cvscore_plot(cvscore_df = mse_df, 
                  score_name = "MSE", 
                  best_lambda = best_lambda, 
                  style = "base") + ggtitle("Ridge regression: MSE path")
```

## Least Absolute Shrinkage and Selector Operator (LASSO)
```{r, fig.width=6}
# Run LASSO regression ---------------------------------------------------------
lasso_out <- cv.glmnet(x = X, 
                       y = y, 
                       lambda = lambdas,
                       penalty.factor = pen_facts,
                       family = "gaussian", 
                       alpha = 1, # 0 for ridge, 1 for lasso, 0.5 for elastic net 
                       standardize = FALSE, 
                       type.measure = "mse",
                       nfolds = 6)


# Collect results into data frames ---------------------------------------------
lambdas <- lasso_out$lambda
cv_scores <- lasso_out$cvm
all_coefs <- as.data.frame(as.matrix(t(lasso_out$glmnet.fit$beta)[,1:ncol(X)]))
colnames(all_coefs) <- colnames(X)
all_coefs <- all_coefs %>%
  mutate(lambda = log(lambdas)) %>%
  gather(covariate, value, -lambda)

mse_df <- data.frame(lambda = log(lambdas),
                     score = cv_scores)
best_lambda <- min(mse_df$lambda[which(mse_df$score == min(mse_df$score))])

# Plot the regularization paths ------------------------------------------------
# These are 'modselr' functions for plotting
make_coef_plot(coef_df = all_coefs, 
               best_lambda = best_lambda, 
               style = "base") + ggtitle("LASSO: coefficient paths")

make_cvscore_plot(cvscore_df = mse_df, 
                  score_name = "MSE", 
                  best_lambda = best_lambda, 
                  style = "base") + ggtitle("LASSO: MSE path")
```

## Elastic net
```{r, fig.width=6}
# Run Elastic Net regression ---------------------------------------------------
enet_out <- cv.glmnet(x = X, 
                       y = y, 
                       lambda = lambdas,
                       penalty.factor = pen_facts,
                       family = "gaussian", 
                       alpha = 0.5, # 0 for ridge, 1 for lasso 
                       standardize = FALSE, 
                       type.measure = "mse",
                       nfolds = 6)


# Collect results into data frames ---------------------------------------------
lambdas <- enet_out$lambda
cv_scores <- enet_out$cvm
all_coefs <- as.data.frame(as.matrix(t(enet_out$glmnet.fit$beta)[,1:ncol(X)]))
colnames(all_coefs) <- colnames(X)
all_coefs <- all_coefs %>%
  mutate(lambda = log(lambdas)) %>%
  gather(covariate, value, -lambda)

mse_df <- data.frame(lambda = log(lambdas),
                     score = cv_scores)
best_lambda <- min(mse_df$lambda[which(mse_df$score == min(mse_df$score))])


# Plot the regularization paths ------------------------------------------------
# These are 'modselr' functions for plotting
make_coef_plot(coef_df = all_coefs, 
               best_lambda = best_lambda, 
               style = "base") + ggtitle("Elastic Net: coefficient paths")

make_cvscore_plot(cvscore_df = mse_df, 
                  score_name = "MSE", 
                  best_lambda = best_lambda, 
                  style = "base") + ggtitle("Elastic Net: MSE path")
```

## Mixed-effects LASSO
As we discussed in the main text, statistical regularization techniques for mixed-effects, or hierarchical, models are still in active development. It seems that LASSO methods have developed most rapidly -- there is even an R package, `glmmLASSO`. We use the package to fit and regularize a mixed effects model where there is a random intercept effect of meadow (`meada` in the `butterfly` dataframe).

Note that `glmmLasso` does not include a cross-validation routine for tuning the penalty parameter (`lambda` in the function). So we built our own CV function for `glmmLasso` (see `modselr/R/lasso_cv.R`), which we use here to create path plots similar to those above and to choose the optimal penalty.

```{r, fig.width=6}
# Load the glmmLasso package ---------------------------------------------------
library(glmmLasso)


# Prepare data for glmmLasso ---------------------------------------------------
all_vars <- c("Rt", "meada", imp_vars) # growth rate and meadow to variable list
fitting_df <- butterfly %>%
  filter(year < 2014) %>%
  select(all_vars) %>%
  mutate(meada = as.factor(meada)) # grouping variable must be a factor




# Example of a glmmLasso call --------------------------------------------------
mixed_lasso <- glmmLasso(fix = Rt ~ novextmax+julraintmn1+novextmin+novmean+
                               decextmax+novmeanmax+logNt+novmeanmin,
                         rnd = list(meada =~ 1),
                         data = fitting_df,
                         lambda = 10, # note the hard set for the penalty
                         family = gaussian(link="identity"),
                         switch.NR=FALSE,
                         final.re=FALSE,
                         control = list())


# Call the CV routine to find optimal penalty ----------------------------------

lambdas <- 10^seq(2, -2, by = -.1) # sequence of penalties to test
nlambdas <- length(lambdas)

fitting_tmp <- butterfly %>%
  filter(year < 2014) %>%
  select(all_vars) %>%
  mutate(meada = as.factor(meada))

fitting_df <- purrr::map_df(seq_len(nlambdas), ~fitting_tmp) %>%
  mutate(
    lambda = rep(lambdas, each = nrow(fitting_tmp)),
    lambda_in = lambda
  ) %>%
  group_by(lambda) %>%
  nest()

# Fit models --------------------------------------------------------------

lasso_model <- function(df) {
  L1 <- unique(df$lambda_in)
  glmmLasso(fix = Rt ~ novextmax+julraintmn1+novextmin+novmean+
                               decextmax+novmeanmax+logNt+novmeanmin,
                         rnd = list(meada =~ 1),
                         data = df,
                         lambda = L1,
                         family = gaussian(link="identity"),
                         switch.NR=FALSE,
                         final.re=FALSE,
                         control = list())
}

models <- fitting_df %>%
  mutate(
    model  = data %>% map(lasso_model)
  )



# keep `year` this time for the making data folds
all_vars <- c("year", "Rt", "meada", imp_vars) # year, growth rate, and meadow to variable list
fitting_df <- butterfly %>%
  filter(year < 2014) %>%
  select(all_vars) %>%
  mutate(meada = as.factor(meada)) # grouping variable must be a factor

allyears <- unique(fitting_df$year)


nfolds <- 6
folds <- cut(seq(1,length(allyears)),breaks=nfolds,labels=FALSE) # get folds
folds_sharp <- sample(folds, length(folds)) # shuffle folds
folds_df <- data.frame(year = allyears,
                       fold_id = folds_sharp)

out_mse <- matrix(data = NA, nrow = length(lambdas), ncol = nfolds)

for(i in 1:length(lambdas)){
  ilambda <- lambdas[i]
  for(ifold in 1:nfolds){
    iyear <- folds_df %>% filter(fold_id == ifold) %>% pull(year)
    fitter <- fitting_df %>% filter(!(year %in% iyear))
    tester <- fitting_df %>% filter(year %in% iyear)
    tmp_lasso <- glmmLasso(fix = Rt ~ novextmax+julraintmn1+novextmin+novmean+
                                   decextmax+novmeanmax+logNt+novmeanmin,
                             rnd = list(meada =~ 1),
                             data = fitter,
                             lambda = ilambda,
                             family = gaussian(link="identity"),
                             switch.NR = FALSE,
                             final.re = FALSE,
                             control = list())
    
    fixed_effects <- tmp_lasso$coefficients
    fixed_matrix <- as.data.frame(matrix(rep(fixed_effects,nrow(tester)), nrow = nrow(tester), byrow = TRUE))
    colnames(fixed_matrix) <- paste0(names(fixed_effects),"_beta")
    rand_effects <- tmp_lasso$ranef
    pred_df <- tester %>%
      # mutate(randoms = rand_effects) %>%
      cbind(fixed_matrix) %>%
      mutate(Rt_hat = novextmax*novextmax_beta +
               julraintmn1*julraintmn1_beta +
               novextmin*novextmin_beta +
               novmean*novmean_beta +
               decextmax*decextmax_beta +
               novmeanmax*novmeanmax_beta +
               logNt*logNt_beta +
               novmeanmin*novmeanmin_beta)
    tmp_mse <- mean((pred_df$Rt - pred_df$Rt_hat)^2)
    # tmpout <- data.frame(lambda = ilambda,
    #                      outyear = iyear,
    #                      mse = tmp_mse)
    out_mse[i,ifold] <- tmp_mse
    
    print(paste("Done with fold",ifold,"for lambda =",ilambda))
  }
}


```
