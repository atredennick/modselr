---
title: "Regularization Examples: Ridge Regression, LASSO, and Elastic Net"
author: "Andrew Tredennick"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    fig_caption: yes
vignette: >
  %\VignetteIndexEntry{Regularization Examples: Ridge Regression, LASSO, and Elastic Net}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This vignette walks through the code used to produce the examples of statistical regularization shown in the paper. We use the `butterfly` data from Roland and Matter 2016 (*Ecological Monographs*, data [here](https://datadryad.org/resource/doi:10.5061/dryad.tp324)). As mentioned in the paper, we focus on **ridge regression**, the **least absolute shrinkage and selector operator** (LASSO), and the **elastic net**, but many other regulators exist, and interested readers should peruse the machine learning literature. Outside of R, there is a comphrensive python library called [scikit-learn](http://scikit-learn.org/stable/index.html) with many model selection routines.

##  Ridge regression
```{r, fig.width=6}
# Set seed and load packages ---------------------------------------------------
set.seed(12300)
library(modselr)
library(tidyverse)
library(glmnet) # package for statistical regularization


# Subset out fitting data ------------------------------------------------------
# Get one subpopulation
sub_pop <- butterfly %>%
  filter(meada == 'L', year < 2014)

# Subset out just the important variables from Roland & Matter (2016) Figure 3
imp_vars <- c("novextmax",
              "julraintmn1",
              "novextmin",
              "novmean",
              "decextmax",
              "novmeanmax",
              "logNt",
              "novmeanmin")


# Prepare data for glmnet ------------------------------------------------------
y <- sub_pop$Rt # population growth rate
X <- as.matrix(select(sub_pop, imp_vars)) # covariate matrix

# Make sure there are no NAs in the covariate matrix
test_vec <- as.numeric(apply(X, MARGIN = 2, FUN = "mean"))
nacols <- which(is.na(test_vec))
if(length(nacols) > 0) X <- X[,-nacols]

# Standaradize the covariate values
X <- scale(X, center = TRUE, scale = TRUE)


# Run ridge regression ---------------------------------------------------------
pen_facts <- rep(1, ncol(X)) # penalize all covariates
lambdas <- 10^seq(2, -2, by = -.005) # sequence of penalties to test

ridge_out <- cv.glmnet(x = X, 
                       y = y, 
                       lambda = lambdas,
                       penalty.factor = pen_facts,
                       family = "gaussian", 
                       alpha = 0, # 0 for ridge, 1 for lasso, 0.5 for elastic net 
                       standardize = FALSE, 
                       type.measure = "mse", # mean square error
                       nfolds = 6) # for cross-validation


# Collect results into data frames ---------------------------------------------
lambdas <- ridge_out$lambda
cv_scores <- ridge_out$cvm
all_coefs <- as.data.frame(as.matrix(t(ridge_out$glmnet.fit$beta)[,1:ncol(X)]))
colnames(all_coefs) <- colnames(X)
all_coefs <- all_coefs %>%
  mutate(lambda = log(lambdas)) %>%
  gather(covariate, value, -lambda)

mse_df <- data.frame(lambda = log(lambdas),
                     score = cv_scores)

best_lambda <- min(mse_df$lambda[which(mse_df$score == min(mse_df$score))])


# Plot the regularization paths ------------------------------------------------
# These are 'modselr' functions for plotting
make_coef_plot(coef_df = all_coefs, 
               best_lambda = best_lambda, 
               style = "base") + ggtitle("Ridge regression: coefficient paths")

make_cvscore_plot(cvscore_df = mse_df, 
                  score_name = "MSE", 
                  best_lambda = best_lambda, 
                  style = "base") + ggtitle("Ridge regression: MSE path")
```

## Least Absolute Shrinkage and Selector Operator (LASSO)
```{r, fig.width=6}
# Run LASSO regression ---------------------------------------------------------
lasso_out <- cv.glmnet(x = X, 
                       y = y, 
                       lambda = lambdas,
                       penalty.factor = pen_facts,
                       family = "gaussian", 
                       alpha = 1, # 0 for ridge, 1 for lasso, 0.5 for elastic net 
                       standardize = FALSE, 
                       type.measure = "mse",
                       nfolds = 6)


# Collect results into data frames ---------------------------------------------
lambdas <- lasso_out$lambda
cv_scores <- lasso_out$cvm
all_coefs <- as.data.frame(as.matrix(t(lasso_out$glmnet.fit$beta)[,1:ncol(X)]))
colnames(all_coefs) <- colnames(X)
all_coefs <- all_coefs %>%
  mutate(lambda = log(lambdas)) %>%
  gather(covariate, value, -lambda)

mse_df <- data.frame(lambda = log(lambdas),
                     score = cv_scores)
best_lambda <- min(mse_df$lambda[which(mse_df$score == min(mse_df$score))])

# Plot the regularization paths ------------------------------------------------
# These are 'modselr' functions for plotting
make_coef_plot(coef_df = all_coefs, 
               best_lambda = best_lambda, 
               style = "base") + ggtitle("LASSO: coefficient paths")

make_cvscore_plot(cvscore_df = mse_df, 
                  score_name = "MSE", 
                  best_lambda = best_lambda, 
                  style = "base") + ggtitle("LASSO: MSE path")
```

## Elastic net
```{r, fig.width=6}
# Run Elastic Net regression ---------------------------------------------------
enet_out <- cv.glmnet(x = X, 
                       y = y, 
                       lambda = lambdas,
                       penalty.factor = pen_facts,
                       family = "gaussian", 
                       alpha = 0.5, # 0 for ridge, 1 for lasso 
                       standardize = FALSE, 
                       type.measure = "mse",
                       nfolds = 6)


# Collect results into data frames ---------------------------------------------
lambdas <- enet_out$lambda
cv_scores <- enet_out$cvm
all_coefs <- as.data.frame(as.matrix(t(enet_out$glmnet.fit$beta)[,1:ncol(X)]))
colnames(all_coefs) <- colnames(X)
all_coefs <- all_coefs %>%
  mutate(lambda = log(lambdas)) %>%
  gather(covariate, value, -lambda)

mse_df <- data.frame(lambda = log(lambdas),
                     score = cv_scores)
best_lambda <- min(mse_df$lambda[which(mse_df$score == min(mse_df$score))])


# Plot the regularization paths ------------------------------------------------
# These are 'modselr' functions for plotting
make_coef_plot(coef_df = all_coefs, 
               best_lambda = best_lambda, 
               style = "base") + ggtitle("Elastic Net: coefficient paths")

make_cvscore_plot(cvscore_df = mse_df, 
                  score_name = "MSE", 
                  best_lambda = best_lambda, 
                  style = "base") + ggtitle("Elastic Net: MSE path")
```
