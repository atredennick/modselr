---
title: "Regularization Examples: Ridge Regression, LASSO, and Elastic Net"
author: "Andrew Tredennick"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    fig_caption: yes
vignette: >
  %\VignetteIndexEntry{Regularization Examples: Ridge Regression, LASSO, and Elastic Net}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This vignette walks through the code used to produce the examples of statistical regularization shown in the paper. We use the `butterfly` data from Roland and Matter 2016 (*Ecological Monographs*, data [here](https://datadryad.org/resource/doi:10.5061/dryad.tp324)). As mentioned in the paper, we focus on **ridge regression**, the **least absolute shrinkage and selector operator** (LASSO), and the **elastic net**, but many other regulators exist, and interested readers should peruse the machine learning literature. Outside of R, there is a comphrensive python library called [scikit-learn](http://scikit-learn.org/stable/index.html) with many model selection routines.

##  Ridge regression
```{r}
library(modselr)
library(tidyverse)
library(glmnet) # package for statistical regularization

# Get one subpopulation
sub_pop <- butterfly %>%
  filter(meada == 'L', Year < 2014)

# Subset out just the important variables from Roland & Matter (2016) Figure 3
imp_vars <- c("novextmax",
              "julraintmn1",
              "novextmin",
              "novmean",
              "decextmax",
              "novmeanmax",
              "logNt",
              "novmeanmin")

# Perform ridge regression
y <- sub_pop$Rt
X <- as.matrix(select(sub_pop, imp_vars))

test_vec <- as.numeric(apply(X, MARGIN = 2, FUN = "mean"))
nacols <- which(is.na(test_vec))
if(length(nacols) > 0) X <- X[,-nacols]

X <- scale(X, center = TRUE, scale = TRUE)

pen_facts <- rep(1, ncol(X))
lambdas <- 10^seq(2, -2, by = -.005)

ridge_out <- cv.glmnet(x = X, 
                       y = y, 
                       lambda = lambdas,
                       penalty.factor = pen_facts,
                       family = "gaussian", 
                       alpha = 0, # 0 for ridge, 1 for lasso 
                       standardize = FALSE, 
                       type.measure = "mse",
                       nfolds = 6)

lambdas <- ridge_out$lambda
cv_scores <- ridge_out$cvm
all_coefs <- as.data.frame(as.matrix(t(ridge_out$glmnet.fit$beta)[,1:ncol(X)]))
colnames(all_coefs) <- colnames(X)
all_coefs <- all_coefs %>%
  mutate(lambda = log(lambdas)) %>%
  gather(variable, coef_value, -lambda)

mse_df <- data.frame(lambda = log(lambdas),
                     mse = cv_scores)
best_lambda <- min(mse_df$lambda[which(mse_df$mse == min(mse_df$mse))])

ggplot(all_coefs, aes(x = lambda, y = coef_value, color = variable))+
  geom_vline(aes(xintercept = best_lambda), color = "grey65", linetype = "dashed")+
  geom_line()+
  ggtitle("Ridge regression: coefficient paths")

ggplot(mse_df, aes(x = lambda, y = mse))+
  geom_vline(aes(xintercept = best_lambda), color = "grey65", linetype = "dashed")+
  geom_line()+
  xlab(expression(log(lambda)))+
  ylab("Cross-validation MSE")+
  ggtitle("Ridge regression: MSE path")
```

## Least Absolute Shrinkage and Selector Operator (LASSO)
```{r}
library(modselr)
library(tidyverse)
library(glmnet) # package for statistical regularization

# Get one subpopulation
sub_pop <- butterfly %>%
  filter(meada == 'L', Year < 2014)

# Subset out just the important variables from Roland & Matter (2016) Figure 3
imp_vars <- c("novextmax",
              "julraintmn1",
              "novextmin",
              "novmean",
              "decextmax",
              "novmeanmax",
              "logNt",
              "novmeanmin")

# Perform ridge regression
y <- sub_pop$Rt
X <- as.matrix(select(sub_pop, imp_vars))

test_vec <- as.numeric(apply(X, MARGIN = 2, FUN = "mean"))
nacols <- which(is.na(test_vec))
if(length(nacols) > 0) X <- X[,-nacols]

X <- scale(X, center = TRUE, scale = TRUE)

pen_facts <- rep(1, ncol(X))
lambdas <- 10^seq(2, -2, by = -.005)

ridge_out <- cv.glmnet(x = X, 
                       y = y, 
                       lambda = lambdas,
                       penalty.factor = pen_facts,
                       family = "gaussian", 
                       alpha = 1, # 0 for ridge, 1 for lasso 
                       standardize = FALSE, 
                       type.measure = "mse",
                       nfolds = 6)

lambdas <- ridge_out$lambda
cv_scores <- ridge_out$cvm
all_coefs <- as.data.frame(as.matrix(t(ridge_out$glmnet.fit$beta)[,1:ncol(X)]))
colnames(all_coefs) <- colnames(X)
all_coefs <- all_coefs %>%
  mutate(lambda = log(lambdas)) %>%
  gather(variable, coef_value, -lambda)

mse_df <- data.frame(lambda = log(lambdas),
                     mse = cv_scores)
best_lambda <- min(mse_df$lambda[which(mse_df$mse == min(mse_df$mse))])

ggplot(all_coefs, aes(x = lambda, y = coef_value, color = variable))+
  geom_vline(aes(xintercept = best_lambda), color = "grey65", linetype = "dashed")+
  geom_line()+
  ggtitle("Ridge regression: coefficient paths")

ggplot(mse_df, aes(x = lambda, y = mse))+
  geom_vline(aes(xintercept = best_lambda), color = "grey65", linetype = "dashed")+
  geom_line()+
  xlab(expression(log(lambda)))+
  ylab("Cross-validation MSE")+
  ggtitle("Ridge regression: MSE path")
```
